{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG6pIZ-_7uRX"
      },
      "source": [
        "## OpenAI's Agents SDK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN2gNfry7tDp"
      },
      "source": [
        "OpenAI have released an **Agents SDK**, their version of an open source agent development library.\n",
        "\n",
        "OpenAI have outlined a few features of the library:\n",
        "\n",
        "```\n",
        "* Agent loop: Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done.\n",
        "* Python-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.\n",
        "* Handoffs: A powerful feature to coordinate and delegate between multiple agents.\n",
        "* Guardrails: Run input validations and checks in parallel to your agents, breaking early if the checks fail.\n",
        "* Function tools: Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation.\n",
        "* Tracing: Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools.\n",
        "```\n",
        "\n",
        "([source](https://openai.github.io/openai-agents-python/))\n",
        "\n",
        "We'll focus on covering the essentials here - including the **agent loop**, **python-first**, **guardrails**, and **function tools** features.\n",
        "\n",
        "Let's start by installing the library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3aqSlMc27noo"
      },
      "outputs": [],
      "source": [
        "!uv pip install -qU openai-agents==0.0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBGHkh7nBVHp"
      },
      "source": [
        "First let's set our [OpenAI API key](https://platform.openai.com/settings/organization/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwpk_H5_BnAl",
        "outputId": "89f732f4-db60-4b68-e937-f95ec84cdaeb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \\\n",
        "  getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lqkaleTAAjHK"
      },
      "outputs": [],
      "source": [
        "from agents import Agent, Runner\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You're a helpful assistant\",\n",
        "    model=\"gpt-4o-mini\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-zOBFhAEROm"
      },
      "source": [
        "## Running our Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWO4ozC8ETNK"
      },
      "source": [
        "OpenAI gives us three methods for running our agent, all via a `Runner` class â€” those methods are:\n",
        "\n",
        "1. `Runner.run()` which runs in async.\n",
        "2. `Runner.run_sync()` which runs in sync.\n",
        "3. `Runner.run_streamed()` which runs in async _and_ streams the response back to us.\n",
        "\n",
        "We'll quicky test method **(1)**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "Bjn-gyR9BIn-",
        "outputId": "4d2f3062-7d78-4dae-8b1e-ecfebb594ba7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Once in a quiet village nestled between rolling hills, there lived an elderly woman named Elara. She was known for her vibrant garden that burst with colors: sunflowers, daisies, and delicate bluebells. Every morning, the villagers would pause to admire her blooms, but none dared to disturb her peaceful routine.\\n\\nOne day, a young boy named Leo, curious and adventurous, decided to trespass into Elara\\'s garden. He had heard tales of the legendary golden flower said to grant wishes. As he tiptoed through the blossoms, he stumbled upon a single golden bloom shining in the early sunlight.\\n\\nBefore he could reach it, Elara appeared, her silver hair glinting in the light. \"What brings you to my garden, young one?\" she asked gently. Leo, startled but brave, confessed his quest.\\n\\nElara smiled and said, \"The golden flower is a treasure, but wishes come with consequences. What do you truly desire?\"\\n\\nAfter pondering, Leo replied, \"I wish for everyone in the village to be happy.\" Elara nodded and plucked the flower, handing it to him. \"Remember, happiness often lies in kindness and connection.\"\\n\\nLeo closed his eyes and made his wish. In that moment, the garden shimmered, and a warm light enveloped the village. From then on, the villagers began to help one another, share laughter, and cherish their bonds.\\n\\nLeo learned that the true magic came not from a single wish, but from the love and kindness they all shared. And Elara smiled, knowing her garden had grown even more vibrant with joy.'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"tell me a short story\"\n",
        ")\n",
        "result.final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5_yue8fE4eX"
      },
      "source": [
        "In most scenarios we'll likely want to be using method **(3)**, ie running async and streaming tokens. To do this we need to write a little more code to handle the async streaming and print the tokens as they're returned.\n",
        "\n",
        "First, we create a `RunResultStreaming` object by calling `Runner.run_streamed(...)`, we then _asynchronously_ iterate through the streamed events returned by our LLM using the `response.stream_events()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBnhUepDBPHM",
        "outputId": "043f936a-7aea-4ec8-fdfd-a7aeb9ea2f39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='resp_6817521042948191b763e3d3bf72c40909443eb4c6b1fc4e', created_at=1746358800.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseInProgressEvent(response=Response(id='resp_6817521042948191b763e3d3bf72c40909443eb4c6b1fc4e', created_at=1746358800.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.in_progress'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_68175210bf548191b26223e77bc5761209443eb4c6b1fc4e', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='msg_68175210bf548191b26223e77bc5761209443eb4c6b1fc4e', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='Hello', item_id='msg_68175210bf548191b26223e77bc5761209443eb4c6b1fc4e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='!', item_id='msg_68175210bf548191b26223e77bc5761209443eb4c6b1fc4e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' How', item_id='msg_68175210bf548191b26223e77bc5761209443eb4c6b1fc4e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' can', item_id='msg_68175210bf548191b26223e77bc5761209443eb4c6b1fc4e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' I', item_id='msg_68175210bf548191b26223e77bc5761209443eb4c6b1fc4e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' assist', item_id='msg_68175210bf548191b26223e77bc5761209443eb4c6b1fc4e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' you', item_id='msg_68175210bf548191b26223e77bc5761209443eb4c6b1fc4e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' today', item_id='msg_68175210bf548191b26223e77bc5761209443eb4c6b1fc4e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='?', item_id='msg_68175210bf548191b26223e77bc5761209443eb4c6b1fc4e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDoneEvent(content_index=0, item_id='msg_68175210bf548191b26223e77bc5761209443eb4c6b1fc4e', output_index=0, text='Hello! How can I assist you today?', type='response.output_text.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='msg_68175210bf548191b26223e77bc5761209443eb4c6b1fc4e', output_index=0, part=ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text'), type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_68175210bf548191b26223e77bc5761209443eb4c6b1fc4e', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='resp_6817521042948191b763e3d3bf72c40909443eb4c6b1fc4e', created_at=1746358800.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_68175210bf548191b26223e77bc5761209443eb4c6b1fc4e', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=17, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=10, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=27), user=None, store=True), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item=ResponseOutputMessage(id='msg_68175210bf548191b26223e77bc5761209443eb4c6b1fc4e', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ],
      "source": [
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"hello there\"\n",
        ")\n",
        "async for event in response.stream_events():\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH-RoQfzMj6T"
      },
      "source": [
        "We can filter these various event types to find only raw tokens like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGQANyvXG9zJ",
        "outputId": "8f64469d-fe14-4d74-be40-6a0d0af51848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once upon a time in a small village nestled between two towering mountains, there lived a young girl named Lila. Lila had a curious spirit and a keen sense of adventure. Every day after finishing her chores, she would wander into the lush forest that bordered her home.\n",
            "\n",
            "One day, while exploring deeper than ever before, she stumbled upon a shimmering pool surrounded by vibrant flowers. The water sparkled under the sunlight, and for a moment, Lila thought she saw a flicker of movement beneath the surface. Intrigued, she knelt beside the pool.\n",
            "\n",
            "Suddenly, a small, luminous fish leaped out of the water and landed on the grass beside her. To her astonishment, it spoke! \"Help me, kind girl! I am under a curse, and only a true act of kindness can set me free.\"\n",
            "\n",
            "Lila, moved by the fish's plight, asked how she could help. \"Return me to the water, and in return, I will grant you a wish,\" it said, its fins shimmering in the sunlight.\n",
            "\n",
            "Without hesitation, Lila scooped the fish up and gently placed it back into the pool. The water glowed, and the fish swirled around joyfully before transforming into a beautiful water sprite. \"Thank you, dear Lila! Your kindness has broken my curse. What is your wish?\"\n",
            "\n",
            "Lila thought for a moment and replied, \"I wish for my village to be filled with kindness, where everyone helps each other.\"\n",
            "\n",
            "The sprite smiled and waved her hands, casting a radiant light over the village. From that day on, the villagers began helping one another, sharing their resources, and spreading joy and compassion.\n",
            "\n",
            "Lila's heart swelled with happiness as she witnessed the change. She knew that true magic came not from wishes but from acts of kindness. And so, she continued her adventures, always ready to lend a helping hand, inspiring others to do the same. In this way, the village flourished, a testament to Lila's unwavering spirit and the power of kindness."
          ]
        }
      ],
      "source": [
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "# we do need to reinitialize our runner before re-executing\n",
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"tell me a short story\"\n",
        ")\n",
        "\n",
        "async for event in response.stream_events():\n",
        "    if event.type == \"raw_response_event\" and \\\n",
        "        isinstance(event.data, ResponseTextDeltaEvent):\n",
        "        print(event.data.delta, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Iixlv0sRMqr"
      },
      "source": [
        "## Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDHORYUcROZn"
      },
      "source": [
        "OpenAI included **function tools** as a key feature in their Agents SDK announcement. After turning everyone away from using _function calling_ to instead use _tool calling_, OpenAI have now decided that an LLM deciding to execute some code will be called _\"function tools\"_.\n",
        "\n",
        "To use _function tools_ in Agents SDK we simply decorate a function with the `@function_tool` decorator like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tJz7BlwiNOa3"
      },
      "outputs": [],
      "source": [
        "from agents import function_tool\n",
        "\n",
        "@function_tool\n",
        "def multiply(x: float, y: float) -> float:\n",
        "    \"\"\"Multiplies `x` and `y` to provide a precise\n",
        "    answer.\"\"\"\n",
        "    return x*y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8b8KektTQbK"
      },
      "source": [
        "Note that we have taken extra care to include a clear and descriptive function name, relatively clear parameter names, type annotations for both input parameters and expected output, and a natural language docstring that will be fed to the LLM and explain to it _what_ this tool does.\n",
        "\n",
        "To run our agent _with_ tools we simply pass our new tool into the `tools` parameter during `Agent` initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WnT4C9WBTO_M"
      },
      "outputs": [],
      "source": [
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=(\n",
        "        \"You're a helpful assistant, remember to always \"\n",
        "        \"use the provided tools whenever possible. Do not \"\n",
        "        \"rely on your own knowledge too much and instead \"\n",
        "        \"use your tools to help you answer queries.\"\n",
        "    ),\n",
        "    model=\"gpt-4o-mini\",\n",
        "    tools=[multiply]  # note that we expect a list of tools\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtioNxflUS-s"
      },
      "source": [
        "Now let's initialize a new runner and execute our agent with tools:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSOREQ-XUSVi",
        "outputId": "579e3dc2-832c-4d35-da30-5bcbd757248b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x10e2179c0>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='resp_68175215f0cc81918f7690dc9bf4c8560d4a61146fa4aa3e', created_at=1746358805.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseInProgressEvent(response=Response(id='resp_68175215f0cc81918f7690dc9bf4c8560d4a61146fa4aa3e', created_at=1746358805.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.in_progress'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseFunctionToolCall(arguments='', call_id='call_Bvc9YFp6BThqOibWUNNk0Fy7', name='multiply', type='function_call', id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', status='in_progress'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='{\"', item_id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='x', item_id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='\":', item_id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='7', item_id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='.', item_id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='814', item_id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta=',\"', item_id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='y', item_id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='\":', item_id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='103', item_id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='.', item_id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='892', item_id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='}', item_id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDoneEvent(arguments='{\"x\":7.814,\"y\":103.892}', item_id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', output_index=0, type='response.function_call_arguments.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseFunctionToolCall(arguments='{\"x\":7.814,\"y\":103.892}', call_id='call_Bvc9YFp6BThqOibWUNNk0Fy7', name='multiply', type='function_call', id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', status='completed'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='resp_68175215f0cc81918f7690dc9bf4c8560d4a61146fa4aa3e', created_at=1746358805.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseFunctionToolCall(arguments='{\"x\":7.814,\"y\":103.892}', call_id='call_Bvc9YFp6BThqOibWUNNk0Fy7', name='multiply', type='function_call', id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=113, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=22, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=135), user=None, store=True), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='tool_called', item=ToolCallItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x10e2179c0>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item=ResponseFunctionToolCall(arguments='{\"x\":7.814,\"y\":103.892}', call_id='call_Bvc9YFp6BThqOibWUNNk0Fy7', name='multiply', type='function_call', id='fc_681752165108819182025b39029436d30d4a61146fa4aa3e', status='completed'), type='tool_call_item'), type='run_item_stream_event')\n",
            "RunItemStreamEvent(name='tool_output', item=ToolCallOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x10e2179c0>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item={'call_id': 'call_Bvc9YFp6BThqOibWUNNk0Fy7', 'output': '811.812088', 'type': 'function_call_output'}, output='811.812088', type='tool_call_output_item'), type='run_item_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='resp_68175216e0f08191bab2b3036283d5ec0d4a61146fa4aa3e', created_at=1746358806.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseInProgressEvent(response=Response(id='resp_68175216e0f08191bab2b3036283d5ec0d4a61146fa4aa3e', created_at=1746358806.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.in_progress'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='The', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' result', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' of', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' multiplying', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' \\\\(', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' ', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='7', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='814', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' \\\\', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=')', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' by', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' \\\\(', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' ', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='103', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='892', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' \\\\', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=')', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' is', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' \\\\(', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' ', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='811', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='812', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='088', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' \\\\', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=').', item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDoneEvent(content_index=0, item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, text='The result of multiplying \\\\( 7.814 \\\\) by \\\\( 103.892 \\\\) is \\\\( 811.812088 \\\\).', type='response.output_text.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', output_index=0, part=ResponseOutputText(annotations=[], text='The result of multiplying \\\\( 7.814 \\\\) by \\\\( 103.892 \\\\) is \\\\( 811.812088 \\\\).', type='output_text'), type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', content=[ResponseOutputText(annotations=[], text='The result of multiplying \\\\( 7.814 \\\\) by \\\\( 103.892 \\\\) is \\\\( 811.812088 \\\\).', type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='resp_68175216e0f08191bab2b3036283d5ec0d4a61146fa4aa3e', created_at=1746358806.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', content=[ResponseOutputText(annotations=[], text='The result of multiplying \\\\( 7.814 \\\\) by \\\\( 103.892 \\\\) is \\\\( 811.812088 \\\\).', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=146, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=30, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=176), user=None, store=True), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x10e2179c0>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item=ResponseOutputMessage(id='msg_68175217225881918e7d5b4fa83350910d4a61146fa4aa3e', content=[ResponseOutputText(annotations=[], text='The result of multiplying \\\\( 7.814 \\\\) by \\\\( 103.892 \\\\) is \\\\( 811.812088 \\\\).', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ],
      "source": [
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"what is 7.814 multiplied by 103.892?\"\n",
        ")\n",
        "\n",
        "async for event in response.stream_events():\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQZu6iyPU0wG"
      },
      "source": [
        "If we look closely at the fourth event object we will see `ResponseFunctionToolCall`, meaning our `multiply` tool was called by our LLM. Following this event object we can also see several events containing the `ResponseFunctionCallArgumentsDeltaEvent` type inside the `data` field â€” these are the input parameters for our tool.\n",
        "\n",
        "Let's rerun that but this time we will process the event outputs to generate a cleaner and more readable output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhFvNVowUrd5",
        "outputId": "c572523c-e86a-4925-9010-367387766b5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Current Agent: Assistant\n",
            "{\"x\":7.814,\"y\":103.892}\n",
            "> Tool Called, name: multiply\n",
            "> Tool Called, args: {\"x\":7.814,\"y\":103.892}\n",
            "> Tool Output: 811.812088\n",
            "The result of multiplying \\( 7.814 \\) by \\( 103.892 \\) is \\( 811.812088 \\)."
          ]
        }
      ],
      "source": [
        "from openai.types.responses import (\n",
        "    ResponseFunctionCallArgumentsDeltaEvent,  # tool call streaming\n",
        "    ResponseCreatedEvent,  # start of new event like tool call or final answer\n",
        ")\n",
        "\n",
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"what is 7.814 multiplied by 103.892?\"\n",
        ")\n",
        "\n",
        "async for event in response.stream_events():\n",
        "    if event.type == \"raw_response_event\":\n",
        "        if isinstance(event.data, ResponseFunctionCallArgumentsDeltaEvent):\n",
        "            # this is streamed parameters for our tool call\n",
        "            print(event.data.delta, end=\"\", flush=True)\n",
        "        elif isinstance(event.data, ResponseTextDeltaEvent):\n",
        "            # this is streamed final answer tokens\n",
        "            print(event.data.delta, end=\"\", flush=True)\n",
        "    elif event.type == \"agent_updated_stream_event\":\n",
        "        # this tells us which agent is currently in use\n",
        "        print(f\"> Current Agent: {event.new_agent.name}\")\n",
        "    elif event.type == \"run_item_stream_event\":\n",
        "        # these are events containing info that we'd typically\n",
        "        # stream out to a user or some downstream process\n",
        "        if event.name == \"tool_called\":\n",
        "            # this is the collection of our _full_ tool call after our tool\n",
        "            # tokens have all been streamed\n",
        "            print()\n",
        "            print(f\"> Tool Called, name: {event.item.raw_item.name}\")\n",
        "            print(f\"> Tool Called, args: {event.item.raw_item.arguments}\")\n",
        "        elif event.name == \"tool_output\":\n",
        "            # this is the response from our tool execution\n",
        "            print(f\"> Tool Output: {event.item.raw_item['output']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqqWET4O93il"
      },
      "source": [
        "## Guardrails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBrf2Ao396Dg"
      },
      "source": [
        "OpenAI have also included guardrails in the Agents SDK. These come as _input guardrails_ and _output guardrails_, the `input_guardrail` checks that the input going into your LLM is \"safe\" and the `output_guardrail` checks that the output from your LLM is \"safe\".\n",
        "\n",
        "Let's see how to use them. First, we'll implement a guardrail powered by another LLM (more tokens means more $$$ for OpenAI)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "I1LT_UpiXFg0"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "# define structure of output for any guardrail agents\n",
        "class GuardrailOutput(BaseModel):\n",
        "    is_triggered: bool\n",
        "    reasoning: str\n",
        "\n",
        "# define an agent that checks if user is asking about political opinions\n",
        "politics_agent = Agent(\n",
        "    name=\"Politics check\",\n",
        "    instructions=\"Check if the user is asking you about political opinions\",\n",
        "    output_type=GuardrailOutput,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRRFcCFGBHSk"
      },
      "source": [
        "We can call this agent directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4kgYMC9A7WP",
        "outputId": "7660cdce-00f3-41c9-812c-691fd8a85919"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RunResult(input='what do you think about the labour party in the UK?', new_items=[MessageOutputItem(agent=Agent(name='Politics check', instructions='Check if the user is asking you about political opinions', handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=<class '__main__.GuardrailOutput'>, hooks=None), raw_item=ResponseOutputMessage(id='msg_6817521aad7c8191a62818dbe327a82e01c351a7edb15570', content=[ResponseOutputText(annotations=[], text='{\"is_triggered\":true,\"reasoning\":\"The user is asking about political opinions regarding the Labour Party in the UK, which explicitly relates to political topics and sentiments.\"}', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item')], raw_responses=[ModelResponse(output=[ResponseOutputMessage(id='msg_6817521aad7c8191a62818dbe327a82e01c351a7edb15570', content=[ResponseOutputText(annotations=[], text='{\"is_triggered\":true,\"reasoning\":\"The user is asking about political opinions regarding the Labour Party in the UK, which explicitly relates to political topics and sentiments.\"}', type='output_text')], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=83, output_tokens=36, total_tokens=119), referenceable_id='resp_6817521a3a388191a6e9ef05296aedca01c351a7edb15570')], final_output=GuardrailOutput(is_triggered=True, reasoning='The user is asking about political opinions regarding the Labour Party in the UK, which explicitly relates to political topics and sentiments.'), input_guardrail_results=[], output_guardrail_results=[], _last_agent=Agent(name='Politics check', instructions='Check if the user is asking you about political opinions', handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=<class '__main__.GuardrailOutput'>, hooks=None))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"what do you think about the labour party in the UK?\"\n",
        "\n",
        "result = await Runner.run(starting_agent=politics_agent, input=query)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NfFXKYOBeJO"
      },
      "source": [
        "The output from our agent is hidden away in there, we extract it like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrS4dT5mC7jN",
        "outputId": "6e464d4f-ecbb-4aed-c0d4-208fcfcc9f17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GuardrailOutput(is_triggered=True, reasoning='The user is asking about political opinions regarding the Labour Party in the UK, which explicitly relates to political topics and sentiments.')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okpF-4pAC_zC"
      },
      "source": [
        "To integrate this with our other agents we need to move our logic into a single function decorated with the `@input_guardrail` decorator.\n",
        "\n",
        "When defining these guardrails we need to follow the following structure:\n",
        "\n",
        "* Input parameters must include a `ctx` (context), `agent`, and `input` (the user's query in this case). Note that below we will only use the `input` parameter.\n",
        "* Output must be a `GuardrailFunctionOutput` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "o3UQIt-qBLTK"
      },
      "outputs": [],
      "source": [
        "from agents import (\n",
        "    GuardrailFunctionOutput,\n",
        "    RunContextWrapper,\n",
        "    input_guardrail\n",
        ")\n",
        "\n",
        "# this is the guardrail function that returns GuardrailFunctionOutput object\n",
        "@input_guardrail\n",
        "async def politics_guardrail(\n",
        "    ctx: RunContextWrapper[None],\n",
        "    agent: Agent,\n",
        "    input: str,\n",
        ") -> GuardrailFunctionOutput:\n",
        "    # run agent to check if guardrail is triggered\n",
        "    response = await Runner.run(starting_agent=politics_agent, input=input)\n",
        "    # format response into GuardrailFunctionOutput\n",
        "    return GuardrailFunctionOutput(\n",
        "        output_info=response.final_output,\n",
        "        tripwire_triggered=response.final_output.is_triggered,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8eCZluTFnYS"
      },
      "source": [
        "Now we can initialize our normal agent with the `input_guardrails` parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_JYELjOSFoED"
      },
      "outputs": [],
      "source": [
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=(\n",
        "        \"You're a helpful assistant, remember to always \"\n",
        "        \"use the provided tools whenever possible. Do not \"\n",
        "        \"rely on your own knowledge too much and instead \"\n",
        "        \"use your tools to help you answer queries.\"\n",
        "    ),\n",
        "    model=\"gpt-4o-mini\",\n",
        "    tools=[multiply],\n",
        "    input_guardrails=[politics_guardrail],  # note this is a list of guardrails\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxYq9Aq0GIv6"
      },
      "source": [
        "Now let's run it! We'll stick with `Runner.run` for the sake of brevity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GKDUcBYMGGcG",
        "outputId": "e512f6d0-fabb-44a0-c5ce-7a591d2824e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The result of multiplying 7.814 by 103.892 is approximately 811.812.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"what is 7.814 multiplied by 103.892?\"\n",
        ")\n",
        "result.final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5sN23B_HcPn"
      },
      "source": [
        "Let's see if our guardrail will trigger:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "-tncyfxYGdRn",
        "outputId": "c35a415e-0d41-487d-9753-cf0151350039"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Guardrail InputGuardrail triggered tripwire\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"what do you think about the labour party in the UK?\"\n",
        ")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvGzaTDJHmts"
      },
      "source": [
        "Great, our guardrail triggered! The `output_guardrail` type is implemented in almost the exact same way, but uses the `@output_guardrail` decorator when defining the guardrail function, and the `output_guardrails` parameter when defining our `Agent`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnVc_II-IcRg"
      },
      "source": [
        "## Conversational Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EV4EtGZIevd"
      },
      "source": [
        "So far we've only seen how to use our agents with single messages. Many use-cases require chat history to make our agents conversational. To implement that we simply provide a list of messages to our `Runner`.\n",
        "\n",
        "Let's see how this works, first we send a single message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "wK558-qII5WA",
        "outputId": "5eaf5b88-18ef-4d4b-82ac-bf8511d519f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I can't store information or remember details for you. However, you can jot it down in a note or a document for future reference!\""
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"remember the number 7.814 for me please\"\n",
        ")\n",
        "result.final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD_kDKxFI5LO"
      },
      "source": [
        "Fortunately, we can help our agent remember this information. We can use the `.to_input_list()` method to format our `result` into a list of messages for our next query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs-buAAvJh8F",
        "outputId": "3b889236-2a26-4c91-bcca-45733518241d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': 'remember the number 7.814 for me please', 'role': 'user'},\n",
              " {'id': 'msg_6817542c3e188191895eb80b686b299408414a468567bb72',\n",
              "  'content': [{'annotations': [],\n",
              "    'text': \"I can't store information or remember details for you. However, you can jot it down in a note or a document for future reference!\",\n",
              "    'type': 'output_text'}],\n",
              "  'role': 'assistant',\n",
              "  'status': 'completed',\n",
              "  'type': 'message'}]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.to_input_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia1n7UG_JkYh"
      },
      "source": [
        "We merge this with our next message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y4WKSTObJokR",
        "outputId": "01331fad-5e8c-4e64-86cd-a3470432d462"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The result of multiplying 7.814 by 103.892 is approximately 811.812.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=result.to_input_list() + [\n",
        "        {\"role\": \"user\", \"content\": \"multiply the last number by 103.892\"}\n",
        "    ]\n",
        ")\n",
        "result.final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bGdZMCLKDMK"
      },
      "source": [
        "It looks like our agent can remember our previous interactions after all!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khGNoTd_KH4k"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFi__V9hKIKQ"
      },
      "source": [
        "That is our rapid-fire overview of OpenAI's new Agents SDK. We've covered most of the essentials here but there are many other features in the library, and many of the features we included here come with plenty of different ways to use. The SDK is already fairly substantial and certainly worth keeping an eye on."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
